{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784',version=1,as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x = mnist.data.reshape(-1,1,28,28)\n",
    "dataset_x = (dataset_x / 255.0)*2 - 1\n",
    "dataset_y = mnist.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(dataset_x)\n",
    "y_train = torch.tensor(dataset_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "LATENT_DIM = 64\n",
    "IN_CHANNELS = 1\n",
    "IM_SIZE = (28,28)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "NUM_SAMPLES = 225\n",
    "NROWS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINING THE GENERATOR CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.latent_dim = LATENT_DIM\n",
    "        self.img_size = IM_SIZE\n",
    "        self.channels = IN_CHANNELS\n",
    "        activation = nn.LeakyReLU()\n",
    "        layers_dim = [self.latent_dim,128,256,512,self.img_size[0]*self.img_size[1]*self.channels]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(layers_dim[i],layers_dim[i+1]),\n",
    "                nn.BatchNorm1d(layers_dim[i+1]) if i != len(layers_dim) - 2 else nn.Identity(),\n",
    "                activation if i != len(layers_dim) - 2 else nn.Tanh() \n",
    "            )\n",
    "\n",
    "            for i in range(len(layers_dim)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self,z):\n",
    "        batch_size = z.shape[0]\n",
    "        out = z.reshape(-1,self.latent_dim)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out = out.reshape(batch_size,self.channels,self.img_size[0],self.img_size[1])\n",
    "        return out\n",
    "    \n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINING THE DISCRIMINATOR CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_size = IM_SIZE\n",
    "        self.channels = IN_CHANNELS\n",
    "        activation = nn.LeakyReLU()\n",
    "        layers_dim = [self.img_size[0]*self.img_size[1]*self.channels,512,256,128,1]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(layers_dim[i],layers_dim[i+1]),\n",
    "                nn.LayerNorm(layers_dim[i+1]) if i != len(layers_dim) - 2 else nn.Identity(),\n",
    "                activation if i != len(layers_dim) - 2 else nn.Identity() \n",
    "            )\n",
    "\n",
    "            for i in range(len(layers_dim)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = x.reshape(-1,self.img_size[0]*self.img_size[1]*self.channels)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_color_channels(images):\n",
    "\n",
    "    device = images.device\n",
    "    \n",
    "    # Generate random tints for the whole batch (batch_size, 1, 1) and broadcast across all images\n",
    "    # Make sure the random tint factors are within a reasonable range\n",
    "    red_tint = torch.rand(images.size(0), 1, 1, device=device) * 0.8 + 0.2  # Random red tint (0.2 to 1.0)\n",
    "    green_tint = torch.rand(images.size(0), 1, 1, device=device) * 0.8 + 0.2  # Random green tint (0.2 to 1.0)\n",
    "    blue_tint = torch.rand(images.size(0), 1, 1, device=device) * 0.8 + 0.2  # Random blue tint (0.2 to 1.0)\n",
    "    \n",
    "    # Apply the tints to the grayscale images (broadcasting over the batch)\n",
    "    red_channel = images * red_tint\n",
    "    green_channel = images * green_tint\n",
    "    blue_channel = images * blue_tint\n",
    "    \n",
    "    # Stack the 3 channels along a new dimension (batch_size, 3, 28, 28)\n",
    "    colored_images = torch.stack([red_channel, green_channel, blue_channel], dim=1)\n",
    "    \n",
    "    # Clip the values to stay within the range [0, 1]\n",
    "    colored_images = torch.clamp(colored_images, 0, 1)\n",
    "    \n",
    "    return colored_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINING THE TRAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(generated_sample_count,generator):\n",
    "    fake_im_noise = torch.rand((NUM_SAMPLES,LATENT_DIM),device=device)\n",
    "    fake_ims = generator(fake_im_noise)\n",
    "    ims = torch.clamp(fake_ims, -1., 1.).detach().cpu()\n",
    "    ims = (ims + 1) / 2\n",
    "    grid = make_grid(ims,nrow=NROWS)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    if not os.path.exists('samples'):\n",
    "        os.makedirs('samples')\n",
    "    img.save('samples/{}.png'.format(generated_sample_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    mnist_dataset = TensorDataset(x_train,y_train)\n",
    "    mnist_loader = DataLoader(mnist_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "    generator = Generator().to(device) # loaded the generator to gpu(if available)\n",
    "    generator.train() #training mode activated for generator\n",
    "\n",
    "    discriminator = Discriminator().to(device) # loaded the discriminator to gpu(if available)\n",
    "    discriminator.train() #training mode activated for discriminator\n",
    "\n",
    "    optimizer_generator = Adam(generator.parameters(), lr=1.5E-4, betas=(0.5,0.999))\n",
    "    optimizer_discriminator = Adam(discriminator.parameters(), lr=1.5E-4, betas=(0.5,0.999))\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    steps = 0\n",
    "    generated_sample_count = 0\n",
    "\n",
    "    for epoch_no in range(NUM_EPOCHS):\n",
    "        print(\"Epoch no - \",epoch_no+1,\"/\",NUM_EPOCHS)\n",
    "        for im,label in tqdm(mnist_loader):\n",
    "            real_ims = im.float().to(device)\n",
    "            batch_size = real_ims.shape[0]\n",
    "\n",
    "\n",
    "            # optimize the discriminator first\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            fake_im_noise = torch.randn((batch_size,LATENT_DIM),device=device)\n",
    "            fake_ims = generator(fake_im_noise) #generator generated images\n",
    "            real_label = torch.ones((batch_size,1), device=device)\n",
    "            fake_label = torch.zeros((batch_size,1),device=device)\n",
    "            disc_real_pred = discriminator(real_ims)\n",
    "            disc_fake_pred = discriminator(fake_ims.detach()) #detach is used to stop the gradient calculation for the generator\n",
    "            disc_real_loss = criterion(disc_real_pred.reshape(-1),real_label.reshape(-1))\n",
    "            disc_fake_loss = criterion(disc_fake_pred.reshape(-1),fake_label.reshape(-1)) \n",
    "            disc_loss = (disc_real_loss + disc_fake_loss)/2\n",
    "            disc_loss.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            # now optimize the Generator\n",
    "            optimizer_generator.zero_grad()\n",
    "            fake_im_noise = torch.randn((batch_size,LATENT_DIM),device=device)\n",
    "            fake_ims = generator(fake_im_noise)  #generator generated images\n",
    "            disc_fake_pred = discriminator(fake_ims)\n",
    "            gen_fake_loss = criterion(disc_fake_pred.reshape(-1),real_label.reshape(-1))\n",
    "            gen_fake_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            #save samples\n",
    "\n",
    "            if (steps % 300) == 0:\n",
    "                with torch.no_grad():\n",
    "                    generator.eval() # switch the generator to evaluation mode\n",
    "                    infer(generated_sample_count,generator)\n",
    "                    generated_sample_count += 1\n",
    "                    generator.train() # again switched to training mode\n",
    "            steps += 1\n",
    "            \n",
    "        torch.save(generator.state_dict(),'generator_ckpt.pth')\n",
    "        torch.save(discriminator.state_dict(),'discriminator_ckpt.pth')\n",
    "\n",
    "    print('Done Training.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no -  1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [00:14<00:00, 75.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no -  2 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [00:14<00:00, 76.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no -  3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 862/1094 [00:12<00:03, 71.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch no - \u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch_no\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,NUM_EPOCHS)\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m im,label \u001b[38;5;129;01min\u001b[39;00m tqdm(mnist_loader):\n\u001b[0;32m     21\u001b[0m         real_ims \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m real_ims\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\envs\\py311torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
